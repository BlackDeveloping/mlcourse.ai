{"cells":[{"metadata":{"_kg_hide-input":false},"cell_type":"markdown","source":"# General information\nThis kernel is dedicated to EDA of [Dota 2 Winner Prediction Competition ](https://www.kaggle.com/c/mlcourse-dota2-win-prediction)\n\nWe are provided with prepared data and described features as well as with a lot of \"raw\" json data. We need to predict winner of the game. \nEvaluation metric is [ROC-AUC](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc). \n\n<left><img src='https://kor.ill.in.ua/m/610x385/1848785.jpg'>"},{"metadata":{"_kg_hide-input":false},"cell_type":"markdown","source":"To simplify you navigation through this kernel: \n\n* [Main data exploration](#maindata)\n  * [Target distribution](#Target)\n  * [General features](#Generalfeatures)\n  * [Coordinates features](#Coordinatesfeatures)\n  * [T-SNE on means coordinates features](#TSNE)\n  * [KDA](#KDA)\n* [Models comparison](#simplemodels)\n* [LGBM feature importance](#FeatureImportance)\n* [Submission](#Submission)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split, ShuffleSplit, KFold, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression,Ridge, RidgeCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\n\nfrom lightgbm import LGBMClassifier\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set()\n\nimport time\nimport datetime\n\n#import shap\n# load JS visualization code to notebook\n#shap.initjs()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nPATH_TO_DATA = '../input/'\n\nsample_submission = pd.read_csv(os.path.join(PATH_TO_DATA, 'sample_submission.csv'), \n                                    index_col='match_id_hash')\ndf_train_features = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_features.csv'), \n                                    index_col='match_id_hash')\ndf_train_targets = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_targets.csv'), \n                                   index_col='match_id_hash')\ndf_test_features = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_features.csv'), \n                                   index_col='match_id_hash')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Main data exploration\n<div id=\"maindata\">\n</div>\n\nIn this part I'll focus on features created by organizers. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_features.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_features.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_targets.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of Training set: {0}\\nShape of Test set: {1}'.format(df_train_features.shape,df_test_features.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, we have almost 40k entries in train dataset and we need to predict results of other 10k battles.\n\n** UPD: ** Thanks to  [@sonfire](https://www.kaggle.com/sonfire), [@ecdrid](https://www.kaggle.com/adityaecdrid) and [@ambisinistra](https://www.kaggle.com/ambisinistra) who helps me to understand some features in `df_train_targets`. <br>\n* `time_remaining` means how much time remains till the end of the game at the point of time at which all characteristics and statistics shown. Indeed, if you'll sum `game_time` and `time_remaining` you receive exactly `duration` of the game. <br>\n* `next_roshan_team` tell us about next team after that point of time which will take roshan.\n\nMaybe I have to read more about Dota, they have competitions with [prizes](http://dota2.prizetrac.kr/) more than on Kaggle. \n\n![Hm](http://img4.wikia.nocookie.net/__cb20150117182228/plantsvszombies/images/5/57/Wait-what.jpg)\n\nJust kiddin :) \n\nLet's continue, first I'll select target and then divide features on groups and observe them and their correllation with target."},{"metadata":{},"cell_type":"markdown","source":"## Target \n<div id=\"Target\">\nAs we know ROC-AUC is almost robust to class imbalance but let's see how it's distributed to better understand data: \n</div>\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = pd.Series(df_train_targets['radiant_win'].map({True: 1, False: 0}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(target);\nplt.title('Target distribution');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## General features\n<div id=\"Generalfeatures\">\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"general_features = ['game_time', 'game_mode', 'lobby_type', 'objectives_len', 'chat_len']\ngen_feat_df = df_train_features[general_features].copy()\ngen_feat_df['target'] = target\nplt.figure(figsize=(8, 5));\nax = sns.heatmap(gen_feat_df.corr(),annot=True,)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just a little notice if you prefer other view of heatmap (check out [documentation](https://seaborn.pydata.org/generated/seaborn.heatmap.html) for more):"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 5));\nmask = np.zeros_like(gen_feat_df.corr())\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    ax = sns.heatmap(gen_feat_df.corr(), mask=mask,annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see correlation between target and general features is low, which seems to be logical. \nGame time or type of the game, as far as I know it, shouldn't affect much on winning side. \nLet's move to more interesting features and first take a look on map: \n\n![Dota 2 Map](https://habrastorage.org/webt/vq/h2/9c/vqh29cm1vd-69blhriyqr98saww.png)\n\nAs description said `The goal is to destroy the opponent's fountain. No draws are possible in Dota 2.` which means that coordinates could be useful.\nLogic is the following: team which is on the enemy fonte at the end of the game won."},{"metadata":{},"cell_type":"markdown","source":"## Coordinates features\n<div id=\"Coordinatesfeatures\">\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Top 10 features correlated with target (abs values):')\nprint(abs(df_train_features.corrwith(target)).sort_values(ascending=False)[0:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I have no idea why there is no single x coordinate feature in top 10. Who have an idea pleas share in comments! "},{"metadata":{"trusted":true},"cell_type":"code","source":"r_y_coord = ['r{0}_y'.format(i) for i in range(1,6)]\nr_x_coord = ['r{0}_x'.format(i) for i in range(1,6)]\nr_coord = r_y_coord+r_x_coord\n\nd_y_coord = ['d{0}_y'.format(i) for i in range(1,6)]\nd_x_coord = ['d{0}_x'.format(i) for i in range(1,6)]\nd_coord = d_y_coord+d_x_coord","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coord_feat_df = df_train_features[r_coord+d_coord].copy()\ncoord_feat_df['target'] = target\nplt.figure(figsize=(16, 10));\nax = sns.heatmap(coord_feat_df.corr(),annot=True,)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here i decided to investigate which exactly values it takes, and was surprised that there is no 0 coordinates: "},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Min y coordinate for Radiant: {0}'.format(coord_feat_df[r_y_coord].min(axis=0).sort_values(ascending=True)[0:1].values))\nprint('Max y coordinate for Radiant: {0}'.format(coord_feat_df[r_y_coord].max(axis=0).sort_values(ascending=False)[0:1].values)) \nprint('Min x coordinate for Radiant: {0}'.format(coord_feat_df[r_x_coord].min(axis=0).sort_values(ascending=True)[0:1].values))\nprint('Max x coordinate for Radiant: {0}'.format(coord_feat_df[r_x_coord].max(axis=0).sort_values(ascending=False)[0:1].values)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Min y coordinate for Dire: {0}'.format(coord_feat_df[d_y_coord].min(axis=0).sort_values(ascending=True)[0:1].values))\nprint('Max y coordinate for Dire: {0}'.format(coord_feat_df[d_y_coord].max(axis=0).sort_values(ascending=False)[0:1].values)) \nprint('Min x coordinate for Dire: {0}'.format(coord_feat_df[d_x_coord].min(axis=0).sort_values(ascending=True)[0:1].values))\nprint('Max x coordinate for Dire: {0}'.format(coord_feat_df[d_x_coord].max(axis=0).sort_values(ascending=False)[0:1].values)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that range for y's finishes is: 116 while for x's: 122. \nThis means map is not completely symmetrical. \nLet's see now how this values differs for Radiant and Dire victories."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#https://stackoverflow.com/questions/38783027/jupyter-notebook-display-two-pandas-tables-side-by-side\nfrom IPython.display import display_html \ndef display_side_by_side(*args):\n    html_str=''\n    for df in args:\n        html_str+=df.to_html()\n    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_side_by_side(coord_feat_df[coord_feat_df['target']==1].describe().T,coord_feat_df[coord_feat_df['target']==0].describe().T)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the correlation table above we see some groups of features. <br>Let's manually unite them using mean.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"coord_feat_df_mean = coord_feat_df.copy()\ncoord_feat_df_mean['target'] = target\n\ncoord_feat_df_mean['r_y_mean'] = coord_feat_df_mean[r_y_coord].mean(axis=1)\ncoord_feat_df_mean['r_x_mean'] = coord_feat_df_mean[r_x_coord].mean(axis=1)\ncoord_feat_df_mean['d_y_mean'] = coord_feat_df_mean[d_y_coord].mean(axis=1)\ncoord_feat_df_mean['d_x_mean'] = coord_feat_df_mean[d_x_coord].mean(axis=1)\nmean_cols = ['r_y_mean', 'r_x_mean', 'd_y_mean', 'd_x_mean']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coord_feat_df_mean.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 5));\nax = sns.heatmap(coord_feat_df_mean[mean_cols+['target']].corr(),annot=True,)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now show how this mean coordinates features corresponds with each other and with target"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns_plot = sns.pairplot(coord_feat_df_mean[mean_cols+['target']])\nsns_plot.savefig('pairplot.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## T-SNE on means coordinates features\n<div id=\"TSNE\">\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This take a lot of time. "},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntsne = TSNE(random_state=17)\ntsne_representation = tsne.fit_transform(coord_feat_df_mean[mean_cols]) #https://habr.com/ru/company/ods/blog/323210/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(tsne_representation[:, 0], tsne_representation[:, 1], \n            c=coord_feat_df_mean['target'].map({0: 'blue', 1: 'orange'}));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that points are cross each other, but there are at least 2 clusters which we could see in top left and top right corner. <br>\nMight try to find this 2 clusters based on mean coordinates features. "},{"metadata":{},"cell_type":"markdown","source":"## KDA (Kills|Deaths|Assists) \n<div id=\"KDA\">\n</div>"},{"metadata":{},"cell_type":"markdown","source":"I will create another separate DataFrame to analyze kills, death and assists."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"r_kills = ['r{0}_kills'.format(i) for i in range(1,6)]\nr_deaths = ['r{0}_deaths'.format(i) for i in range(1,6)]\nr_assists = ['r{0}_assists'.format(i) for i in range(1,6)]\nr_kda = r_kills+r_deaths+r_assists\n\nd_kills = ['d{0}_kills'.format(i) for i in range(1,6)]\nd_deaths = ['d{0}_deaths'.format(i) for i in range(1,6)]\nd_assists = ['d{0}_assists'.format(i) for i in range(1,6)]\nd_kda = d_kills+d_deaths+d_assists\n\nkda_feat_df = df_train_features[r_kda+d_kda].copy()\nkda_feat_df['target'] = target\n\nkda_feat_df['r_tot_kills'] = kda_feat_df[r_kills].sum(axis=1)\nkda_feat_df['r_tot_deaths'] = kda_feat_df[r_deaths].sum(axis=1)\nkda_feat_df['r_tot_assists'] = kda_feat_df[r_assists].sum(axis=1)\n\nkda_feat_df['d_tot_kills'] = kda_feat_df[d_kills].sum(axis=1)\nkda_feat_df['d_tot_deaths'] = kda_feat_df[d_deaths].sum(axis=1)\nkda_feat_df['d_tot_assists'] = kda_feat_df[d_assists].sum(axis=1)\n\ntot_cols = ['r_tot_kills', 'r_tot_deaths', 'r_tot_assists', 'd_tot_kills', 'd_tot_deaths', 'd_tot_assists']\n\ndisplay(kda_feat_df.head(3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 5));\nax = sns.heatmap(kda_feat_df[tot_cols+['target']].corr(),annot=True,)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"KDA in dota is [calculated](https://steamcommunity.com/app/570/discussions/0/3307213006841396427/ ) as: (K+A)/D \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"kda_feat_df['r_kda'] = (kda_feat_df['r_tot_kills']+kda_feat_df['r_tot_assists'])/kda_feat_df['r_tot_deaths']\nkda_feat_df['d_kda'] = (kda_feat_df['d_tot_kills']+kda_feat_df['d_tot_assists'])/kda_feat_df['d_tot_deaths']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(4.8, 3));\nax = sns.heatmap(kda_feat_df[['r_kda','d_kda','target']].corr(),annot=True,)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Other feauteres could be analyzed in the same way. <br>\nEven more data is stored in JSON files. <br>\nNow let's implement and compare few models. "},{"metadata":{},"cell_type":"markdown","source":"# Simple models comparison\n<div id=\"simplemodels\">\nFirst, I am preparing data for learning and setting cross validation\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_train_features\ny = df_train_targets['radiant_win']\nX_test = df_test_features\ny_cat = pd.Series(df_train_targets['radiant_win'].map({True: 1, False: 0})) #catboost doesn't understand True,False \n#X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=17) #for holdout, don't use in kernel\nn_fold = 5\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=11)\ncv = ShuffleSplit(n_splits=n_fold, test_size=0.3, random_state=17) #same as in https://www.kaggle.com/c/mlcourse-dota2-win-prediction/kernels starter kernel ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And now will use following models without hyperparams: \n\nRF, LGBM, XGB, CatBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nmodel_rf = RandomForestClassifier(n_estimators=100, n_jobs=4,\n                                   max_depth=None, random_state=17)\n\n# calcuate ROC-AUC for each split\ncv_scores_rf = cross_val_score(model_rf, X, y, cv=cv, scoring='roc_auc')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nmodel_lgb = LGBMClassifier(random_state=17)\ncv_scores_lgb = cross_val_score(model_lgb, X, y, cv=cv, \n                                scoring='roc_auc', n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nmodel_xgb = xgb.XGBClassifier(random_state=17)\ncv_scores_xgb = cross_val_score(model_xgb, X, y, cv=cv,\n                                scoring='roc_auc', n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next cell runs ~ 13 min (it freezes completely with n_jobs not equal to 1 for unknown reason). <br>\nIt's definitely better and faster to use native CatBoost CV than `sklearn` one. <br>\nYou could check my [kernel](https://www.kaggle.com/vchulski/catboost-and-shap-for-dota-2-winner-prediction) dedicated to CatBoost."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \nmodel_cat = CatBoostClassifier(random_state=17,silent=True)\ncv_scores_cat = cross_val_score(model_cat, X, y_cat, cv=cv,\n                                scoring='roc_auc', n_jobs=1) #pay attention n_jobs=1 here, just freezes with any other value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_results = pd.DataFrame(data={'RF': cv_scores_rf, 'LGB':cv_scores_lgb, 'XGB':cv_scores_xgb, 'CAT':cv_scores_cat})\ndisplay_side_by_side(cv_results, cv_results.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see CatBoost gives best results among tested algorithms - but it's very rough comparison.After adding hyperparams this could change.\nAnyway, this gives some hint which models we definetly should try. \n\nAlso, we see how fast LGBM is. I am not counting CatBoost which I will test later, but it's even 2 times faster than RF. "},{"metadata":{},"cell_type":"markdown","source":"# Feature Importance\n<div id=\"FeatureImportance\">\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#just visit https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n#https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc\nparams = {#'num_leaves': 31, # number of leaves in full tree (31 by default) \n         'learning_rate': 0.01, #this determines the impact of each tree on the final outcome. \n\n         'min_data_in_leaf': 50,\n         'min_sum_hessian_in_leaf': 12.0,\n         'objective': 'binary', \n         'max_depth': -1,\n         'boosting': 'gbdt', #'dart' \n         'bagging_freq': 5,\n         'bagging_fraction': 0.81,\n         'boost_from_average':'false',\n         'bagging_seed': 17,\n         'metric': 'auc',\n         'verbosity': -1,\n         }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# this part is based on great kernel https://www.kaggle.com/artgor/seismic-data-eda-and-baseline by @artgor\noof = np.zeros(len(X))\nprediction = np.zeros(len(X_test))\nscores = []\nfeature_importance = pd.DataFrame()\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n    print('Fold', fold_n, 'started at', time.ctime())\n    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    model = LGBMClassifier(**params, n_estimators = 2000, nthread = 5, n_jobs = -1)\n    model.fit(X_train, y_train, \n              eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='auc',\n              verbose=200, early_stopping_rounds=200)\n            \n    y_pred_valid = model.predict_proba(X_valid)[:, 1]\n    y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)[:, 1]\n        \n    oof[valid_index] = y_pred_valid.reshape(-1,)\n    scores.append(roc_auc_score(y_valid, y_pred_valid))\n    prediction += y_pred    \n    \n    # feature importance\n    fold_importance = pd.DataFrame()\n    fold_importance[\"feature\"] = X.columns\n    fold_importance[\"importance\"] = model.feature_importances_\n    fold_importance[\"fold\"] = fold_n + 1\n    feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\nprediction /= n_fold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \nfeature_importance[\"importance\"] /= n_fold\n    \ncols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n            by=\"importance\", ascending=False)[:50].index\n\nbest_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\nplt.figure(figsize=(14, 16));\nsns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\nplt.title('LGB Features (avg over folds)');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it seems that gold is main feature in the game. \n\nBut pay attention, results can be different based on parameters. \nCheck out [this](https://www.kaggle.com/shokhan/lightgbm-starter-code) kernel for instance. "},{"metadata":{},"cell_type":"markdown","source":"# Submission\n<div id=\"Submission\">\n</div>"},{"metadata":{},"cell_type":"markdown","source":"I won't share blend solution and that's why I will public simple LGB with almost random parameters, but I bet you will do better than me :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb = LGBMClassifier(random_state=17)\nlgb.fit(X, y)\n\nX_test = df_test_features.values\ny_test_pred = lgb.predict_proba(X_test)[:, 1]\ndf_submission = pd.DataFrame({'radiant_win_prob': y_test_pred}, \n                                 index=df_test_features.index)\nsubmission_filename = 'lgb_{}.csv'.format(\n    datetime.datetime.now().strftime('%Y-%m-%d_%H-%M'))\ndf_submission.to_csv(submission_filename)\nprint('Submission saved to {}'.format(submission_filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission.head() #just to check that everything allright ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's my first ever public kernel on Kaggle so any feedback is appreciated."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}