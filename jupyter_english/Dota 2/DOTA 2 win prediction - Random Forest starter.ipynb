{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "540935bf571b28452d474fd9191d8e1e20d87005"
   },
   "source": [
    "# <center> Dota 2 winner prediction\n",
    "\n",
    "<img src='https://habrastorage.org/webt/ua/vn/pq/uavnpqfoih4zwwznvxubu33ispy.jpeg'>\n",
    "\n",
    "#### <center> Originally done by Peter Romov, translated and adapted by Yury Kashnitskiy (@yorko)\n",
    "    \n",
    "### Quick start\n",
    "\n",
    "Grab features prepared by organizers, train a model and submit. \n",
    "\n",
    "1. [Data description](#Data-description)\n",
    "2. [Features created by organizers](#Features-created-by-organizers)\n",
    "3. [Training and evaluating a model](#Training-and-evaluating-a-model)\n",
    "4. [Preparing a submission](#Preparing-a-submission)\n",
    "\n",
    "### Now do it as a real Data Scientist\n",
    "\n",
    "5. [Cross-validation](#Cross-validation)\n",
    "6. [Working with all available information on Dota games](#Working-with-all-available-information-on-Dota-games)\n",
    "7. [Feature engineering](#Feature-engineering)\n",
    "8. [How to build initial features from scratch](#How-to-build-initial-features-from-scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4339c93b125e5ec10e2fb6e4d5d00892570a2911"
   },
   "source": [
    "## Data description\n",
    "\n",
    "We have the following files:\n",
    "\n",
    "- `sample_submission.csv`: example of a submission file\n",
    "- `train_matches.jsonl`, `test_matches.jsonl`: full \"raw\" training data \n",
    "- `train_features.csv`, `test_features.csv`: features created by organizers\n",
    "- `train_targets.csv`: results of training games (including the winner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2df6f18c884cc8dfa75b584d7f71f7d0e89db897"
   },
   "source": [
    "## Features created by organizers\n",
    "\n",
    "These are basic features which include simple players' statistics. Scroll to the end to see how to build these features from raw json files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "14467d4ccb21360c1d699d0275b87f234f30c961"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "PATH_TO_DATA = '../input/'\n",
    "\n",
    "df_train_features = pd.read_csv(os.path.join(PATH_TO_DATA, \n",
    "                                             'train_features.csv'), \n",
    "                                    index_col='match_id_hash')\n",
    "df_train_targets = pd.read_csv(os.path.join(PATH_TO_DATA, \n",
    "                                            'train_targets.csv'), \n",
    "                                   index_col='match_id_hash')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e805d116c02fb349617562accd0b784e199f30dd"
   },
   "source": [
    "We have ~ 40k games, each described by `match_id_hash` (game id) and 245 features. Also `game_time` is given - time (in secs) when the game was over. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3b93c28dfd711f49e4f3a2374175f979547f4b67"
   },
   "outputs": [],
   "source": [
    "df_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c5de043331cca15e917bc20e4bb8e6bed75ddcc9"
   },
   "outputs": [],
   "source": [
    "df_train_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2ab261af9db00f17af31a68c950912c03ab40ae0"
   },
   "source": [
    "We are interested in the `radiant_win` column in `train_targets.csv`. All these features are not known during the game (they come \"from future\" as compared to `game_time`), so we have these features only for training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd4f390eb21611fb9ce7dd2379923238ba6c692b"
   },
   "outputs": [],
   "source": [
    "df_train_targets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c25fbce112c73f64c0fd544c853084e5734de11d"
   },
   "source": [
    "## Training and evaluating a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c679d807c3609bf57320c331fefe93bffc72d948"
   },
   "source": [
    "#### Let's construct a feature matrix `X` and a target vector `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "37ca1c0b98702d304f90d462379fa62f03571ebc"
   },
   "outputs": [],
   "source": [
    "X = df_train_features.values\n",
    "y = df_train_targets['radiant_win'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "135d1642aa2fb6865f874b0f64689b5cb6037491"
   },
   "source": [
    "#### Perform  a train/test split (a simple validation scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "58485053cb79d517d709ceaa3e6e31d1d0b8e0f2"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n",
    "                                                      test_size=0.3, \n",
    "                                                      random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1062cc9f81b5ef07b03385994fd3d6bbe2ade8bd"
   },
   "source": [
    "#### Train the Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "98687be065f843aa0dd201cb9d20b797584c38f2"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100, n_jobs=4, random_state=17)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8b1799aff8d7493557d9bede120ea67af00b14f6"
   },
   "source": [
    "#### Make predictions for the holdout set\n",
    "\n",
    "We need to predict probabilities of class 1 - that Radiant wins, thus we need index 1 in the matrix returned by the `predict_proba` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bc1b7df6bf6bbd7910a947de14e3b7d8e2890e1a"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict_proba(X_valid)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "54947aa18a6e9ab0209fc9134085235bd6a6d810"
   },
   "source": [
    "Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ce5d9759ac0bda3920ed526e2d61a7369a3dca1e"
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0195d46c9fc6fc2ab1ab263ab560ddcbfd4bfea7"
   },
   "source": [
    "#### Let's evaluate prediction quality with the holdout set\n",
    "\n",
    "We'll calculate ROC-AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9ae476b3bd627fb668ca57b42c5b2cf107444fe3"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "valid_score = roc_auc_score(y_valid, y_pred)\n",
    "print('Validation ROC-AUC score:', valid_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0741b9f8ef727db4f0049af76c672c7d7ab94aa0"
   },
   "source": [
    "Out if curiosiry, we can calculate accuracy of a classifier which predicts class 1 if predicted probability is higher than 50%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fb163a80ec9fc92bc2c6356a9193a5356596a42a"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "valid_accuracy = accuracy_score(y_valid, y_pred > 0.5)\n",
    "print('Validation accuracy of P>0.5 classifier:', valid_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "68b784d190c7fccd394e67b7bc20f6dc58c57c5d"
   },
   "source": [
    "## Preparing a submission\n",
    "\n",
    "Now the same for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bf9749b3551b6bcf1e8ce4bea73c32ca5bde1c32"
   },
   "outputs": [],
   "source": [
    "df_test_features = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_features.csv'), \n",
    "                                   index_col='match_id_hash')\n",
    "\n",
    "X_test = df_test_features.values\n",
    "y_test_pred = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "df_submission = pd.DataFrame({'radiant_win_prob': y_test_pred}, \n",
    "                                 index=df_test_features.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8c13fa4c8690e51045e59040f4ef9a621434716c"
   },
   "outputs": [],
   "source": [
    "df_submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "438f19efd3f319b862b73c908e91317f5acf1df2"
   },
   "source": [
    "Save the submission file, it's handy to include current datetime in the filename. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e4a5ac4e81c5fef0a93d1fa54415bb52727489f3"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "submission_filename = 'submission_{}.csv'.format(\n",
    "    datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "df_submission.to_csv(submission_filename)\n",
    "print('Submission saved to {}'.format(submission_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7aa8cfa79efd9253299c1bc52e1778a3660c7ec6"
   },
   "source": [
    "## Cross-validation\n",
    "\n",
    "As we already know, cross-validation is a more reliable validation technique than just one train/test split. Here we'll resort to `ShuffleSplit` to create 5 70%/30% splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c19342d24b586d208d4aa167e6730f1c531dca32"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit, KFold\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dacc8bc320a3417b30fceff7e57d8dd954a9b138"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7e5e235b8d0b084fe5c3c3354e55eda59047f3fc"
   },
   "source": [
    "#### Run cross-validation\n",
    "\n",
    "We'll train 2 versions of the  `RandomForestClassifier` model - first with default capacity (trees are not limited in depth), second - with `min_samples_leaf`=3, i.e. each leave is obliged to have at least 3 instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0abb5505ab9dc21c389235fde06b16e1faca4ec5"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_rf1 = RandomForestClassifier(n_estimators=100, n_jobs=4,\n",
    "                                   max_depth=None, random_state=17)\n",
    "\n",
    "# calcuate ROC-AUC for each split\n",
    "cv_scores_rf1 = cross_val_score(model_rf1, X, y, cv=cv, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "211c1fe30c37a1529bcbd829019fcc2d9c13ca09"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_rf2 = RandomForestClassifier(n_estimators=100, n_jobs=4,\n",
    "                                   min_samples_leaf=3, random_state=17)\n",
    "\n",
    "cv_scores_rf2 = cross_val_score(model_rf2, X, y, cv=cv, \n",
    "                                scoring='roc_auc', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5dd9a0adcc5855ff2b44f1d73045fc9204d104d9"
   },
   "source": [
    "#### CV results\n",
    "\n",
    "The result returned by `cross_val_score` is an array with metric values (ROC-AUC) for each split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d7675ba721f98ea79fda43fb9d2afb76d9dd89e2"
   },
   "outputs": [],
   "source": [
    "cv_scores_rf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "88967b7133942b2e22c9902e28ad6ed6f94be4c9"
   },
   "outputs": [],
   "source": [
    "cv_scores_rf2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2f2983d499791228c2ed99e5559c8808be7d74bd"
   },
   "source": [
    "Let's compare average ROC-AUC among all splits for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d218659e343fa67d972c829b4f982d80e4c7bddf"
   },
   "outputs": [],
   "source": [
    "print('Model 1 mean score:', cv_scores_rf1.mean())\n",
    "print('Model 2 mean score:', cv_scores_rf2.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "45cab0431a5ed4ebf76fe95c126a21e9edacab34"
   },
   "source": [
    "The second model is preferred. Look, there's a caveat here: the second model is actually better for 4 splits out of 5. So if we were to perform only one train/test split, there would've been a 20% probability to make a wrong conclusion that the first model is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0fab6c78d32dc46c36fee562e7edffe503e35d12"
   },
   "outputs": [],
   "source": [
    "cv_scores_rf2 > cv_scores_rf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f1efeae942898ed6ef194ca7c1932070c2ffbc29"
   },
   "source": [
    "## Working with all available information on Dota games\n",
    "Raw data descriptions for all games are given in files `train_matches.jsonl` and `test_matches.jsonl`. Each file has one entry for each game in [JSON](https://en.wikipedia.org/wiki/JSON) format. You only need to know that it can be easily converted to Python objects via the `json.loads` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b95a79b85bc5051773938feb1636d527726f6f18"
   },
   "source": [
    "##### Let's explore a single entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "50dd106f366c8c3fff052d16d3e93f9f54edb8cf"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(PATH_TO_DATA, 'train_matches.jsonl')) as fin:\n",
    "    # read the 18-th line\n",
    "    for i in range(18):\n",
    "        line = fin.readline()\n",
    "    \n",
    "    # read JSON into a Python object \n",
    "    match = json.loads(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c572d6a50cb0f5f7654207806e7d497821832480"
   },
   "source": [
    "The `match` object is now a big Python dictionary. In `match['players']` we have a description of each player.\n",
    "\n",
    "You might think that this `match` object look ugly. You're right! That's actually the real data. And it's the ability to extract nice features from raw data that makes good Data Scientists stand out. You might even be unfamiliar with Dota (or any other application domain) but still be able to construct a good model via feature engineering. It's art and craftmanship at the same time.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c6d35e3ba574467684aa365c84c0a950adb25f8d"
   },
   "outputs": [],
   "source": [
    "#match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "babbafcee99d96b9d685261efee6fa3d4a232293"
   },
   "source": [
    "#### Player description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6c40c80c2e7cd446b5f5cfcf66046750039d13ac"
   },
   "outputs": [],
   "source": [
    "player = match['players'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3ea80ba2f482597b65990c4628c154cb13a2ef64"
   },
   "source": [
    "KDA: the number of kills, deaths, and assists to alleys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8581f49ec3600141b6441a126e5f8a6bf57ec760"
   },
   "outputs": [],
   "source": [
    "player['kills'], player['deaths'], player['assists']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b81e530220044d369f43beb5171287372f5fda94"
   },
   "source": [
    "Some statistics on player abilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fd30e63960bd5348a5c05345cde2c1f2ac96d809"
   },
   "outputs": [],
   "source": [
    "player['ability_uses']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1b4f45c1478a3f1574e3d174a5fd14748ebda83c"
   },
   "source": [
    "#### Example: time series for each player's gold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4fa2367e05b0c8ae6701e61dd0b9f2017b5fc60a"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8eba3e7148640a62fa055bd303161a6773a5e0e7"
   },
   "outputs": [],
   "source": [
    "for player in match['players']:\n",
    "    plt.plot(player['times'], player['gold_t'])\n",
    "    \n",
    "plt.title('Gold change for all players');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a4d99ef0ff061b26f25448d08913ee9990fbe475"
   },
   "source": [
    "#### Function to read files with game descriptions\n",
    "\n",
    "The following function `read_matches(filename)`, can be used to read raw data on Dota 2 games.\n",
    "\n",
    "We recommend to install two Python packages: `ujson` and `tqdm`, it'll make the execution faster and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1fc978d50288806b0b998859c390e60fe42fba56"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    import ujson as json\n",
    "except ModuleNotFoundError:\n",
    "    import json\n",
    "    print ('Please install ujson to read JSON oblects faster')\n",
    "    \n",
    "try:\n",
    "    from tqdm import tqdm_notebook\n",
    "except ModuleNotFoundError:\n",
    "    tqdm_notebook = lambda x: x\n",
    "    print ('Please install tqdm to track progress with Python loops')\n",
    "\n",
    "def read_matches(matches_file):\n",
    "    \n",
    "    MATCHES_COUNT = {\n",
    "        'test_matches.jsonl': 10000,\n",
    "        'train_matches.jsonl': 39675,\n",
    "    }\n",
    "    _, filename = os.path.split(matches_file)\n",
    "    total_matches = MATCHES_COUNT.get(filename)\n",
    "    \n",
    "    with open(matches_file) as fin:\n",
    "        for line in tqdm_notebook(fin, total=total_matches):\n",
    "            yield json.loads(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4008ec8a83b4cbfc7fe1a981cc61574d83d9a29e"
   },
   "source": [
    "#### Reading data in a loop\n",
    "\n",
    "Reading data on all games might take some 2-3 minutes. Thus you'd better stick to the following approach:\n",
    "\n",
    "1. Read a small amount (10-100) of games\n",
    "2. Write code to extract features from these JSON objects\n",
    "3. Make sure the code works fine\n",
    "4. Run the code with all available data\n",
    "5. Save results to a `pickle` file so that you don't need to run all computations from scratch next time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d47ae0ff04e2c5e04719f6e1be0feeaa4cf2d75a"
   },
   "outputs": [],
   "source": [
    "for match in read_matches(os.path.join(PATH_TO_DATA, 'train_matches.jsonl')):\n",
    "    match_id_hash = match['match_id_hash']\n",
    "    game_time = match['game_time']\n",
    "    \n",
    "    # processing each game\n",
    "    \n",
    "    for player in match['players']:\n",
    "        pass  # processing each player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fca6a1e2b7c6eae4788d0abcff705396526a3c4a"
   },
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a9c0e139f42fce6950ec2f33c6812647561475da"
   },
   "outputs": [],
   "source": [
    "def add_new_features(df_features, matches_file):\n",
    "    \n",
    "    # Process raw data and add new features\n",
    "    for match in read_matches(matches_file):\n",
    "        match_id_hash = match['match_id_hash']\n",
    "\n",
    "        # Counting ruined towers for both teams\n",
    "        radiant_tower_kills = 0\n",
    "        dire_tower_kills = 0\n",
    "        for objective in match['objectives']:\n",
    "            if objective['type'] == 'CHAT_MESSAGE_TOWER_KILL':\n",
    "                if objective['team'] == 2:\n",
    "                    radiant_tower_kills += 1\n",
    "                if objective['team'] == 3:\n",
    "                    dire_tower_kills += 1\n",
    "\n",
    "        # Write new features\n",
    "        df_features.loc[match_id_hash, 'radiant_tower_kills'] = radiant_tower_kills\n",
    "        df_features.loc[match_id_hash, 'dire_tower_kills'] = dire_tower_kills\n",
    "        df_features.loc[match_id_hash, 'diff_tower_kills'] = radiant_tower_kills - dire_tower_kills\n",
    "        \n",
    "        # ... here you can add more features ...\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6dc7d1cda24bc8688ab0df396980d14aefa7af8a"
   },
   "outputs": [],
   "source": [
    "# copy the dataframe with features\n",
    "df_train_features_extended = df_train_features.copy()\n",
    "\n",
    "# add new features\n",
    "add_new_features(df_train_features_extended, \n",
    "                 os.path.join(PATH_TO_DATA, \n",
    "                              'train_matches.jsonl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e7c67141278ba210dc18f4a4b75e5147159a44c9"
   },
   "source": [
    "We see new features added to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a09555f39a291782b185cca7e58159689c25d561"
   },
   "outputs": [],
   "source": [
    "df_train_features_extended.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "07076a419cbecd481d206a3924d4db4a6735ae52"
   },
   "source": [
    "#### Evaluating new features\n",
    "\n",
    "Let's run cross-validation with a fixed model but with two different datasets:\n",
    "\n",
    "1. with features built by organizers (base)\n",
    "2. with new features that we've added (extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "14b4f56ff1479c21917e067e4bed4bd0fd604086"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, n_jobs=4, random_state=17)\n",
    "\n",
    "cv_scores_base = cross_val_score(model, X, y, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "cv_scores_extended = cross_val_score(model, df_train_features_extended.values, y, \n",
    "                                     cv=cv, scoring='roc_auc', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "47613ffbe2de38dbfbf20959990f0df98c32043c"
   },
   "outputs": [],
   "source": [
    "print('Base features: mean={} scores={}'.format(cv_scores_base.mean(), \n",
    "                                                cv_scores_base))\n",
    "print('Extended features: mean={} scores={}'.format(cv_scores_extended.mean(), \n",
    "                                                    cv_scores_extended))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4af6c9646fdbfb393fd81fb1d34445ed88dc0d7c"
   },
   "outputs": [],
   "source": [
    "cv_scores_extended > cv_scores_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ca0c4a713dceb7837a4f47f0f39bfdda702d9fe2"
   },
   "source": [
    "As we see, `RandomForestClassifier` shows better cross-validation results in case of the extended dataset. Looks reasonable, that's what we build features for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "423f4d57a418fb4292a856f629fceda01711a631"
   },
   "source": [
    "#### New submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a4e6149a632036e73f91c167ce4ad22d8d28a7f8"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Build the same features for the test set\n",
    "df_test_features_extended = df_test_features.copy()\n",
    "add_new_features(df_test_features_extended, \n",
    "                 os.path.join(PATH_TO_DATA, 'test_matches.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7f672377fb6b06b19ce2192f9f53be41bd39feb8"
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, n_jobs=4, random_state=17)\n",
    "model.fit(X, y)\n",
    "df_submission_base = pd.DataFrame(\n",
    "    {'radiant_win_prob': model.predict_proba(df_test_features.values)[:, 1]}, \n",
    "    index=df_test_features.index,\n",
    ")\n",
    "df_submission_base.to_csv('submission_base_rf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e9caad309603522b61973b8f1d1c66ad77a29a06"
   },
   "outputs": [],
   "source": [
    "model_extended = RandomForestClassifier(n_estimators=100, n_jobs=4, random_state=17)\n",
    "model_extended.fit(df_train_features_extended.values, y)\n",
    "df_submission_extended = pd.DataFrame(\n",
    "    {'radiant_win_prob': model_extended.predict_proba(df_test_features_extended.values)[:, 1]}, \n",
    "    index=df_test_features.index,\n",
    ")\n",
    "df_submission_extended.to_csv('submission_extended_rf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one will be used as a final submission in this kernel\n",
    "!cp submission_extended_rf.csv submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c90b7da55ba79c13d25ac030400036f98b87492"
   },
   "source": [
    "## How to build initial features from scratch\n",
    "\n",
    "Now we diclose the code that we used to build initial features `train_features.csv` and `test_features.csv`. You can modify the following code to add more features.\n",
    "\n",
    "In a nutshell:\n",
    "\n",
    "1. the  `extract_features_csv(match)` function extracts features from game descriptions and writes them into a dictionary\n",
    "2. the `extract_targets_csv(match, targets)` function extracts the target variable `radiant_win`\n",
    "3. iterating through the file with raw data, we collect all features\n",
    "4. with `pandas.DataFrame.from_records()` we create dataframes with new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7a786e30a4266f8e076972cf3657765dd2339f2f"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "MATCH_FEATURES = [\n",
    "    ('game_time', lambda m: m['game_time']),\n",
    "    ('game_mode', lambda m: m['game_mode']),\n",
    "    ('lobby_type', lambda m: m['lobby_type']),\n",
    "    ('objectives_len', lambda m: len(m['objectives'])),\n",
    "    ('chat_len', lambda m: len(m['chat'])),\n",
    "]\n",
    "\n",
    "PLAYER_FIELDS = [\n",
    "    'hero_id',\n",
    "    \n",
    "    'kills',\n",
    "    'deaths',\n",
    "    'assists',\n",
    "    'denies',\n",
    "    \n",
    "    'gold',\n",
    "    'lh',\n",
    "    'xp',\n",
    "    'health',\n",
    "    'max_health',\n",
    "    'max_mana',\n",
    "    'level',\n",
    "\n",
    "    'x',\n",
    "    'y',\n",
    "    \n",
    "    'stuns',\n",
    "    'creeps_stacked',\n",
    "    'camps_stacked',\n",
    "    'rune_pickups',\n",
    "    'firstblood_claimed',\n",
    "    'teamfight_participation',\n",
    "    'towers_killed',\n",
    "    'roshans_killed',\n",
    "    'obs_placed',\n",
    "    'sen_placed',\n",
    "]\n",
    "\n",
    "def extract_features_csv(match):\n",
    "    row = [\n",
    "        ('match_id_hash', match['match_id_hash']),\n",
    "    ]\n",
    "    \n",
    "    for field, f in MATCH_FEATURES:\n",
    "        row.append((field, f(match)))\n",
    "        \n",
    "    for slot, player in enumerate(match['players']):\n",
    "        if slot < 5:\n",
    "            player_name = 'r%d' % (slot + 1)\n",
    "        else:\n",
    "            player_name = 'd%d' % (slot - 4)\n",
    "\n",
    "        for field in PLAYER_FIELDS:\n",
    "            column_name = '%s_%s' % (player_name, field)\n",
    "            row.append((column_name, player[field]))\n",
    "            \n",
    "    return collections.OrderedDict(row)\n",
    "    \n",
    "def extract_targets_csv(match, targets):\n",
    "    return collections.OrderedDict([('match_id_hash', match['match_id_hash'])] + [\n",
    "        (field, targets[field])\n",
    "        for field in ['game_time', 'radiant_win', 'duration', 'time_remaining', 'next_roshan_team']\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b64cc62ea488cab9be4a75ec510918b8d6ca147a"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_new_features = []\n",
    "df_new_targets = []\n",
    "\n",
    "for match in read_matches(os.path.join(PATH_TO_DATA, 'train_matches.jsonl')):\n",
    "    match_id_hash = match['match_id_hash']\n",
    "    features = extract_features_csv(match)\n",
    "    targets = extract_targets_csv(match, match['targets'])\n",
    "    \n",
    "    df_new_features.append(features)\n",
    "    df_new_targets.append(targets)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "69b242ad4d20c20621e515c64b4e457f5faabbb4"
   },
   "outputs": [],
   "source": [
    "df_new_features = pd.DataFrame.from_records(df_new_features).set_index('match_id_hash')\n",
    "df_new_targets = pd.DataFrame.from_records(df_new_targets).set_index('match_id_hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "daa4cf4256d7dc400bde1889bb4e23bf0dc80ab4"
   },
   "outputs": [],
   "source": [
    "df_new_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8dee537e854b63f3eddac74af0a64d02a0bfe45b"
   },
   "source": [
    "## Go on!\n",
    "\n",
    "- Discuss new ideas in Slack \n",
    "- Create new features\n",
    "- Try new models and ensembles\n",
    "- Submit predictions\n",
    "- Go and win!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
